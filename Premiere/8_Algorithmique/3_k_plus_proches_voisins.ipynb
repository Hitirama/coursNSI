{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Accueil](../../index.ipynb) > [Sommaire Première](../index.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Algorithmique : k plus proches voisins\n",
    "\n",
    "## L'apprentissage automatique\n",
    "\n",
    "L'apprentissage automatique (*Machine Learning*) est une branche de l'intelligence artificielle qui se fonde sur une approche mathématique et statistique afin de permettre aux ordinateurs d'apprendre à partir de données.\n",
    "\n",
    "\n",
    "On distingue 2 phases:\n",
    "\n",
    "1. La phase d'**apprentissage** qui consite à alimenter un modèle de données. \n",
    "2. La mise en production lors de laquelle de nouvelles données sont soumises et doivent conduire au résultat souhaité.\n",
    "\n",
    "On distingue différents types d'apprentissages:\n",
    "\n",
    "- L'apprentissage **supervisé** : dans ce cas les données fournies sont étiquetées. Le système apprend à classer selon une classification\n",
    "- L'apprentissage **non supervisé** : les données fournies ne pas sont étiquetées. L'algorithme doit découvrir par lui-même la structure des données.\n",
    "- L'apprentissage **par renforcement** : Un système de récompense/punition en fonction des réponses données permet d'améliorer ses choix. Au début l'algorithme joue au hasard, puis au fur et à mesure utilise ce qu'il a appris pour progresser\n",
    "- L'apprentissage **par transfert** : utiliser des compétences déjà assimilées sur de nouvelles tâches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principe de l'algorithme des k plus proches voisins\n",
    "\n",
    "L'algorithme des **k plus proches voisins** est un algorithme très connu dans le milieu de l'intelligence artificiel. Sa popularité tient à sa facilité de compréhension ainsi qu'à sa mise en oeuvre. De plus la précision des résultats obtenus est de même niveau, voire meilleure, que des algorithmes plus complexe.\n",
    "\n",
    "L'**algorithme des k plus proches voisins** ( *KNN* : *K-Nearest Neighbor* ) est de type **apprentissage supervisé**.\n",
    "\n",
    "La phase d'apprentissage est très rapide puisqu'elle met en mémoire l'ensemble des données d'entrainement pour les comparer avec les données à traiter. Le principal problème est justement la taille mémoire importante ainsi que la complexité temporelle de la phase de tests.\n",
    "\n",
    "On dispose d'un ensemble E de n données labélisées $E=\\{(y_i, \\overrightarrow{x_i})\\}$ avec $1 \\leq i \\leq n$ où \n",
    "- $y_i$ est la **classe** (ou label) de la donnée $i$.\n",
    "- le vecteur $\\overrightarrow{x_i}$ représente les **variables** de la donnée $i$.\n",
    "\n",
    "On considère une donnée $u$ dont on ne connait pas la classe mais dont on dispose des données. on a donc $u = (?, \\overrightarrow{x_u})$\n",
    "\n",
    "On considère également une fonction $d$ qui retourne la **distance** entre u et n'importe quelle donnée appartenant à $E$.\n",
    "\n",
    "Le principe de l'algorithme knn est le suivant:\n",
    "\n",
    "- On calcule toutes les distances entre $u$ et les données de $E$;\n",
    "- On retient les $k$ données de $E$ les plus proches de $u$;\n",
    "- On attribut à $u$ la classe la plus fréquente parmi les $k$ données. \n",
    "\n",
    "<figure>\n",
    "   <img src=\"img/knn_schema.png\"  alt=\"principe de l'algorithme knn\" title=\"principe de l'algorithme knn\">\n",
    "   <figcaption>principe de l'algorithme knn</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Avec un exemple concret la définition mathméatique précédente sera plus claire...**\n",
    "\n",
    "Considérons 3 sortes d'Iris:\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "       <td>\n",
    "           <figure>\n",
    "               <img src=\"img/setosa.jpg\"  alt=\"Iris Setosa\" title=\"Iris Setosa\">\n",
    "               <figcaption>Iris Setosa</figcaption>\n",
    "           </figure>\n",
    "       </td>\n",
    "       <td>\n",
    "           <figure>\n",
    "               <img src=\"img/versicolor.jpg\"  alt=\"Iris Versicolor\" title=\"Iris Versicolor\">\n",
    "               <figcaption>Iris Versicolor</figcaption>\n",
    "           </figure>\n",
    "       </td>\n",
    "       <td>\n",
    "           <figure>\n",
    "               <img src=\"img/virginica.jpg\"  alt=\"Iris Virginica\" title=\"Iris Virginica\">\n",
    "               <figcaption>Iris Virginica</figcaption>\n",
    "           </figure>\n",
    "       </td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "Le **nom des Iris** est la **classe** (ou le label) d'une donnée.\n",
    "\n",
    "Notre ensemble $E$ possède donc trois classes:\n",
    "\n",
    "1. Iris Setosa\n",
    "2. Iris Versicolor\n",
    "3. Iris Virginica\n",
    "\n",
    "Des mesures ont été effectuées sur chaque Iri:\n",
    "\n",
    "- La longueur du sépale;\n",
    "- la largeur du sépale;\n",
    "- la longeur du pétale;\n",
    "- la largeur du pétale.\n",
    "\n",
    "Ces données, appelées les [iris de Fischer](https://fr.wikipedia.org/wiki/Iris_de_Fisher) ont été collectées par [Ronal Fischer](https://fr.wikipedia.org/wiki/Ronald_Aylmer_Fisher) (biologiste et statisticien) en 1936 et contiennent 50 échantillons de chacune des trois espèces.\n",
    "\n",
    "Ces données sont facilement disponibles sur internet et sont souvent utilisées dans un but pédagogique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "url = \"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "data = pd.read_csv(response)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons uniquement la première donnée de notre ensemble $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette donnée, qui appartient à l'ensemble $E$, peut s'écrire $(setosa, (5.1, 3.5, 1.4, 0.2))$\n",
    "\n",
    "Nous disposons \n",
    "\n",
    "- d'un ensemble $E$ de 150 données **labélisées** dont le vecteur des variables est de dimension 4.\n",
    "- d'une donnée non labélisée dont le vecteur des variables est connu.\n",
    "\n",
    "Le vecteur des variables est de dimension 4\n",
    "\n",
    "Commençons par utiliser 2 diagrammes en deux dimensions pour se représenter ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\", symbol=\"species\", width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "fig = px.scatter(df, x=\"petal_width\", y=\"petal_length\", color=\"species\", symbol=\"species\", width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">En observant ces deux diagrammes, quelles observations pouvez vous faire ?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de représenter ces 4 dimensions sur un seul diagramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_3d(df, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_length\", size=\"petal_width\", symbol=\"species\", color=\"species\", width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df, hue = 'species', corner=True, vars = ['sepal_width', 'sepal_length', 'petal_width', 'petal_length',])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">Retrouve-t-on les mêmes observations sur ce diagramme ?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliqueer notre algorithme knn il ne nous manque plus qu'une fonction qui permet de calculer la distance entre notre donnée non labélisée et chaque donnée de $E$.\n",
    "\n",
    "## Les distances\n",
    "\n",
    "<img src=\"img/distances.jpg\">\n",
    "\n",
    "Afin que l'algorithme des plus proches voisins fonctionne pour un jeu de données particulier, il est nécessaire de choisir le **type de distance** la plus appropriée pour ce jeu de données.\n",
    "\n",
    "La **distance euclidienne** est la plus commune mais il en existe bien d'autres.\n",
    "\n",
    "### La distance de Minkowski\n",
    "\n",
    "La distance de Minkowski est une distance générale à partir de laquelle on peut obtenir deux types de distance.\n",
    "\n",
    "Soient deux points A et B de coordonnées respectives $A(a_1, a_2, a_3, ....., a_n)$ et $B(b_1, b_2, b_3, ....., b_n)$\n",
    "\n",
    "$d= (\\sum_{i=1}^{n} |{{a_i - b_i}|^p})^{\\frac{1}{p}}$\n",
    "\n",
    "Ne soyez pas effrayé par cette formule, elle en fait très simple.\n",
    "\n",
    "1. si p=1, la formule devient : $d= (\\sum_{i=1}^{n} |{{a_i - b_i}|^1})^{\\frac{1}{1}}$ soit $d= \\sum_{i=1}^{n} |{{a_i - b_i}|}$ c'est à dire $d = |a_1 - b_1| + |a_2 - b_2| + ... + |a_n - b_n|$. Il s'agit ici de **la distance de Manhantan**. [Lien wikipedia](https://fr.wikipedia.org/wiki/Distance_de_Manhattan)\n",
    "\n",
    "<figure>\n",
    "   <img src=\"img/Manhattan_distance.svg.png\"  alt=\"Distance de Manhattan (chemins rouge, jaune et bleu) contre distance euclidienne en vert.\" title=\"Distance de Manhattan (chemins rouge, jaune et bleu) contre distance euclidienne en vert.\">\n",
    "   <figcaption>Distance de Manhattan (chemins rouge, jaune et bleu) contre distance euclidienne en vert.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "2. si p=2, la formule devient : $d= \\sum_{i=1}^{n} {{|a_i - b_i|^2}}^{\\frac{1}{2}}$ soit $d = \\sqrt{(\\sum_{i=1}^{n} |{{a_i - b_i}|^2})}$ c'est à dire $d = \\sqrt{{(a_1 - b_1)^2+ (a_2 - b_2)^2 + ... +(a_n - b_n)^2}}$. On reconnait alors la formule de la **distance euclidienne** entre deux points.\n",
    "\n",
    "\n",
    "### D'autres distances\n",
    "\n",
    "Il existe de nombreuses distances qui sont adaptées à des jeux de données spécifiques:\n",
    "\n",
    "- La similarité cosinus : utilisée dans les problèmes financiers ou dans la recherche textuelle de similarités (détection de plagia). [Lien Wikipedia](https://fr.wikipedia.org/wiki/Similarit%C3%A9_cosinus)\n",
    "- La distance de Hamming : utilisée pour la comparaison binaire de chaînes de caractères. [Lien Wikipedia](https://fr.wikipedia.org/wiki/Distance_de_Hamming)\n",
    "- ...\n",
    "\n",
    "<div class=\"alert alert-warning\">Dans les exemples que nous traiterons nous utiliserons la <b>distance euclidienne.</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de scikit-learn\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) est une bibliothèque d'intelligence artificielle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des iris de Fisher\n",
    "\n",
    "Cette bibliothèque possède, comme plotly, la base de données des iris de Fisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "# affichage des données\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# affichage des variables\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Affichage des especes\n",
    "# 0 : setosa, 1:versicolor, 2:virginica\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Découpage en deux jeux de données\n",
    "\n",
    "Nous allons diviser ces données en deux parties\n",
    "1. une partie test : 30% des données (voir code ci-dessous)\n",
    "2. Une partie apprentissage (*train*) : 70% des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "# Observons notre jeu de tests\n",
    "print(f\"Les variables de test sont de la forme {X_test.shape}\")\n",
    "print(f\"Les espèces du jeu de test sont : {y_test}\")\n",
    "print(\"Nous avons désormais un jeu d'apprentissage de 105 données et un jeu de test de 45 données.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des variables\n",
    "\n",
    "Nous avons vu que l'algorithme knn utilise une distance pour évaluer les plus proches voisins. Le problème est que souvent certaines variables possèdent des différences plus fortes que d'autres. Ces variables 'comptent' donc plus dans la distance.\n",
    "\n",
    "Il est donc nécessaire de normaliser les variables avant d'effectuer le calcul des longueurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler= Normalizer().fit(X_train) # the scaler is fitted to the training set\n",
    "normalized_X_train= scaler.transform(X_train) # the scaler is applied to the training set\n",
    "normalized_X_test= scaler.transform(X_test) # the scaler is applied to the test set\n",
    "\n",
    "print('x train before Normalization')\n",
    "print(X_train[0:5])\n",
    "print('\\nx train after Normalization')\n",
    "print(normalized_X_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détermination de la valeur k\n",
    "\n",
    "Avant d'utiliser l'algorithme knn il est indispensable de connaitre la valeur k que nous allons appliquer.\n",
    "\n",
    "#### Eviter les multiples\n",
    "\n",
    "Si les données possède deux labels, nous prendrons k impair afin de ne pas avoir d'égalité k = 2n+1\n",
    "\n",
    "Dans notre exemple d'Iris, nous avons trois labels, nous éviterons donc les multiples de 3. Nous prendrons par exemple k = 3n + 1 (1, 4, 7, 10...)\n",
    "\n",
    "#### Détermination par les données\n",
    "\n",
    "Le principe est assez simple : nous allons utiliser des données dont le label est connu (notre jeu d'entrainement )et vérifier avec le jeu de test le taux d'erreur pour plusieurs valeurs de k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation de l'algorithme knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "scores = {}\n",
    "for k in range(1, 15, 3):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(normalized_X_train, y_train)\n",
    "    y_prediction = knn.predict(normalized_X_test)\n",
    "    scores[k] = metrics.accuracy_score(y_test, y_prediction)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les scores sous forme graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = scores.keys(), y = scores.values(), width=400, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">Nous prenons le plus petit k qui donne les meilleurs résultats.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédictions\n",
    "\n",
    "Ajoutons une donnée sans label dans notre base et visualisons la.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "unknown_value = {\"sepal_width\":[3,],\n",
    "                         \"sepal_length\":[6,],\n",
    "                         \"petal_length\":[4.8,],\n",
    "                         \"petal_width\":[1.6,],\n",
    "                         \"species\":['unknown'],\n",
    "                         \"species_id\":[4,],                        \n",
    "                        }\n",
    "\n",
    "# On ajoute une nouvelle donnée de classe inconnue\n",
    "df1 = df = pd.DataFrame(unknown_value)\n",
    "\n",
    "df2 = px.data.iris()\n",
    "# On concatene les 2 dataframe\n",
    "df3 = pd.concat([df2, df1], ignore_index=True)\n",
    "fig = px.scatter_3d(df3, x=\"sepal_width\", y=\"sepal_length\", z=\"petal_length\", size=\"petal_width\", symbol=\"species\", color=\"species\", width=800, height=800)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">En vous inspirant du code de l'entrainement, ecrire une fonction prediction(dico) qui retourne la prediction d'appartenance à une des trois espèces d'iris.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webographie\n",
    "\n",
    "- https://www.kdnuggets.com/2020/11/most-popular-distance-metrics-knn.html\n",
    "- https://towardsdatascience.com/knn-in-python-835643e2fb53\n",
    "- https://medium.com/analytics-vidhya/titanic-machine-learning-by-k-nearest-neighbors-knn-algorithm-530d8bdd8323\n",
    "- https://deepnote.com/@ndungu/Implementing-KNN-Algorithm-on-the-Iris-Dataset-e7c16493-500c-4248-be54-9389de603f16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP Titanic\n",
    "\n",
    "[TP Titanic](../TPs/Titanic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recupération de données sur le web : https://gist.github.com/curran/a08a1080b88344b0c8a7#file-iris-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://snap.stanford.edu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
